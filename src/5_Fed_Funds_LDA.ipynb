{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "import pickle\n",
    "import gensim\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim import corpora, models\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "import string\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "from itertools import chain\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tqdm import tqdm\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import datetime as dt\n",
    "import os\n",
    "from dateutil.relativedelta import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install gensim \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    \n",
    "    exclude = set(string.punctuation)\n",
    "    stop = set(stopwords.words('english'))\n",
    "    lemma = WordNetLemmatizer()\n",
    "    \n",
    "    text = re.sub('[^A-z,.-]+', ' ', text)\n",
    "\n",
    "    stop_free = ' '.join([word for word in text.lower().split() if word not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = ' '.join([lemma.lemmatize(word) for word in punc_free.split()])\n",
    "    return normalized.split()\n",
    "\n",
    "def bigrams(words, bi_min=15, tri_min=10):\n",
    "    bigram = gensim.models.Phrases(words, min_count = bi_min)\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    return bigram_mod\n",
    "    \n",
    "def get_corpus(text_clean):\n",
    "    bigram_mod = bigrams(text_clean)\n",
    "    bigram = [bigram_mod[review] for review in text_clean]\n",
    "    id2word = gensim.corpora.Dictionary(bigram)\n",
    "    id2word.filter_extremes(no_below=10, no_above=0.35)\n",
    "    id2word.compactify()\n",
    "    corpus = [id2word.doc2bow(text) for text in bigram]\n",
    "    return corpus, id2word, bigram\n",
    "\n",
    "\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    from gensim.models.ldamodel import LdaModel\n",
    "    \"\"\"\n",
    "    # From https://datascienceplus.com/evaluation-of-topic-modeling-topic-coherence/\n",
    "    \n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model=LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, passes = 4)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(396, 57)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unscheduled</th>\n",
       "      <th>forecast</th>\n",
       "      <th>confcall</th>\n",
       "      <th>Rate</th>\n",
       "      <th>RateDiff</th>\n",
       "      <th>RateDecision</th>\n",
       "      <th>ChairPerson</th>\n",
       "      <th>RateChanged</th>\n",
       "      <th>GDP_date</th>\n",
       "      <th>GDP_value</th>\n",
       "      <th>...</th>\n",
       "      <th>Hsales_diff_year</th>\n",
       "      <th>Taylor</th>\n",
       "      <th>Balanced</th>\n",
       "      <th>Inertia</th>\n",
       "      <th>Taylor-Rate</th>\n",
       "      <th>Balanced-Rate</th>\n",
       "      <th>Inertia-Rate</th>\n",
       "      <th>Taylor_diff</th>\n",
       "      <th>Balanced_diff</th>\n",
       "      <th>Inertia_diff</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1982-10-05</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>9.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1982-04-01</td>\n",
       "      <td>6830.251</td>\n",
       "      <td>...</td>\n",
       "      <td>42.307692</td>\n",
       "      <td>7.428054</td>\n",
       "      <td>4.300007</td>\n",
       "      <td>7.854999</td>\n",
       "      <td>-2.571946</td>\n",
       "      <td>-5.699993</td>\n",
       "      <td>-2.145001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982-11-16</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1982-07-01</td>\n",
       "      <td>6804.139</td>\n",
       "      <td>...</td>\n",
       "      <td>34.831461</td>\n",
       "      <td>6.397952</td>\n",
       "      <td>2.690992</td>\n",
       "      <td>7.671351</td>\n",
       "      <td>-3.102048</td>\n",
       "      <td>-6.809008</td>\n",
       "      <td>-1.828649</td>\n",
       "      <td>-1.030102</td>\n",
       "      <td>-1.609015</td>\n",
       "      <td>-0.183648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982-12-21</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1982-07-01</td>\n",
       "      <td>6804.139</td>\n",
       "      <td>...</td>\n",
       "      <td>45.026178</td>\n",
       "      <td>6.456348</td>\n",
       "      <td>2.749388</td>\n",
       "      <td>6.812592</td>\n",
       "      <td>-2.043652</td>\n",
       "      <td>-5.750612</td>\n",
       "      <td>-1.687408</td>\n",
       "      <td>0.058396</td>\n",
       "      <td>0.058396</td>\n",
       "      <td>-0.858759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983-01-14</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1982-07-01</td>\n",
       "      <td>6804.139</td>\n",
       "      <td>...</td>\n",
       "      <td>14.004376</td>\n",
       "      <td>6.117343</td>\n",
       "      <td>2.410384</td>\n",
       "      <td>6.863442</td>\n",
       "      <td>-2.382657</td>\n",
       "      <td>-6.089616</td>\n",
       "      <td>-1.636558</td>\n",
       "      <td>-0.339005</td>\n",
       "      <td>-0.339005</td>\n",
       "      <td>0.050851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983-01-21</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1982-07-01</td>\n",
       "      <td>6804.139</td>\n",
       "      <td>...</td>\n",
       "      <td>14.004376</td>\n",
       "      <td>6.117343</td>\n",
       "      <td>2.410384</td>\n",
       "      <td>6.863442</td>\n",
       "      <td>-2.382657</td>\n",
       "      <td>-6.089616</td>\n",
       "      <td>-1.636558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            unscheduled  forecast  confcall  Rate  RateDiff  RateDecision  \\\n",
       "date                                                                        \n",
       "1982-10-05        False     False     False   9.5      -0.5            -1   \n",
       "1982-11-16        False     False     False   9.0      -0.5            -1   \n",
       "1982-12-21        False     False     False   8.5       0.0             0   \n",
       "1983-01-14        False     False      True   8.5       0.0             0   \n",
       "1983-01-21        False     False      True   8.5       0.0             0   \n",
       "\n",
       "           ChairPerson  RateChanged   GDP_date  GDP_value  ...  \\\n",
       "date                                                       ...   \n",
       "1982-10-05         NaN            1 1982-04-01   6830.251  ...   \n",
       "1982-11-16         NaN            1 1982-07-01   6804.139  ...   \n",
       "1982-12-21         NaN            0 1982-07-01   6804.139  ...   \n",
       "1983-01-14         NaN            0 1982-07-01   6804.139  ...   \n",
       "1983-01-21         NaN            0 1982-07-01   6804.139  ...   \n",
       "\n",
       "            Hsales_diff_year    Taylor  Balanced   Inertia  Taylor-Rate  \\\n",
       "date                                                                      \n",
       "1982-10-05         42.307692  7.428054  4.300007  7.854999    -2.571946   \n",
       "1982-11-16         34.831461  6.397952  2.690992  7.671351    -3.102048   \n",
       "1982-12-21         45.026178  6.456348  2.749388  6.812592    -2.043652   \n",
       "1983-01-14         14.004376  6.117343  2.410384  6.863442    -2.382657   \n",
       "1983-01-21         14.004376  6.117343  2.410384  6.863442    -2.382657   \n",
       "\n",
       "            Balanced-Rate Inertia-Rate  Taylor_diff  Balanced_diff  \\\n",
       "date                                                                 \n",
       "1982-10-05      -5.699993    -2.145001          NaN            NaN   \n",
       "1982-11-16      -6.809008    -1.828649    -1.030102      -1.609015   \n",
       "1982-12-21      -5.750612    -1.687408     0.058396       0.058396   \n",
       "1983-01-14      -6.089616    -1.636558    -0.339005      -0.339005   \n",
       "1983-01-21      -6.089616    -1.636558     0.000000       0.000000   \n",
       "\n",
       "            Inertia_diff  \n",
       "date                      \n",
       "1982-10-05           NaN  \n",
       "1982-11-16     -0.183648  \n",
       "1982-12-21     -0.858759  \n",
       "1983-01-14      0.050851  \n",
       "1983-01-21      0.000000  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load nontext data\n",
    "file = open('../data/preprocessed/nontext_data.pickle', 'rb')\n",
    "nontext_data = pd.read_pickle(file)\n",
    "file.close()\n",
    "\n",
    "print(nontext_data.shape)\n",
    "nontext_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9744, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>speaker</th>\n",
       "      <th>word_count</th>\n",
       "      <th>decision</th>\n",
       "      <th>rate</th>\n",
       "      <th>next_meeting</th>\n",
       "      <th>next_decision</th>\n",
       "      <th>next_rate</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>statement</td>\n",
       "      <td>1994-02-04</td>\n",
       "      <td>FOMC Statement</td>\n",
       "      <td>Alan Greenspan</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>3.25</td>\n",
       "      <td>1994-02-28</td>\n",
       "      <td>0</td>\n",
       "      <td>3.25</td>\n",
       "      <td>Chairman Alan Greenspan announced today that t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>statement</td>\n",
       "      <td>1994-03-22</td>\n",
       "      <td>FOMC Statement</td>\n",
       "      <td>Alan Greenspan</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1994-04-18</td>\n",
       "      <td>1</td>\n",
       "      <td>3.75</td>\n",
       "      <td>Chairman Alan Greenspan announced today that t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>statement</td>\n",
       "      <td>1994-04-18</td>\n",
       "      <td>FOMC Statement</td>\n",
       "      <td>Alan Greenspan</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>3.75</td>\n",
       "      <td>1994-05-17</td>\n",
       "      <td>1</td>\n",
       "      <td>4.25</td>\n",
       "      <td>Chairman Alan Greenspan announced today that t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>statement</td>\n",
       "      <td>1994-05-17</td>\n",
       "      <td>FOMC Statement</td>\n",
       "      <td>Alan Greenspan</td>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>4.25</td>\n",
       "      <td>1994-07-06</td>\n",
       "      <td>0</td>\n",
       "      <td>4.25</td>\n",
       "      <td>In taking the discount action, the Board appro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>statement</td>\n",
       "      <td>1994-08-16</td>\n",
       "      <td>FOMC Statement</td>\n",
       "      <td>Alan Greenspan</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>4.75</td>\n",
       "      <td>1994-09-27</td>\n",
       "      <td>0</td>\n",
       "      <td>4.75</td>\n",
       "      <td>In taking the discount rate action, the Board ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        type       date           title         speaker  word_count decision  \\\n",
       "0  statement 1994-02-04  FOMC Statement  Alan Greenspan          99        1   \n",
       "1  statement 1994-03-22  FOMC Statement  Alan Greenspan          40        1   \n",
       "2  statement 1994-04-18  FOMC Statement  Alan Greenspan          37        1   \n",
       "3  statement 1994-05-17  FOMC Statement  Alan Greenspan          57        1   \n",
       "4  statement 1994-08-16  FOMC Statement  Alan Greenspan          51        1   \n",
       "\n",
       "   rate next_meeting next_decision  next_rate  \\\n",
       "0  3.25   1994-02-28             0       3.25   \n",
       "1   3.5   1994-04-18             1       3.75   \n",
       "2  3.75   1994-05-17             1       4.25   \n",
       "3  4.25   1994-07-06             0       4.25   \n",
       "4  4.75   1994-09-27             0       4.75   \n",
       "\n",
       "                                                text  \n",
       "0  Chairman Alan Greenspan announced today that t...  \n",
       "1  Chairman Alan Greenspan announced today that t...  \n",
       "2  Chairman Alan Greenspan announced today that t...  \n",
       "3  In taking the discount action, the Board appro...  \n",
       "4  In taking the discount rate action, the Board ...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load text data\n",
    "file = open('../data/preprocessed/text_no_split.pickle', 'rb')\n",
    "text_data = pd.read_pickle(file)\n",
    "file.close()\n",
    "\n",
    "print(text_data.shape)\n",
    "text_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review text types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "meeting_script     8653\n",
       "speech              439\n",
       "minutes             223\n",
       "statement           195\n",
       "testimony           185\n",
       "presconf_script      49\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Frequency of text-data type\n",
    "text_data.type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fd3e0f797c0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAev0lEQVR4nO3de3wV5b3v8c+PuygQgUChiIEWFYGQYEA5CIfLISgqWKu1SE+hiFTECrwOHKE93upls089asGtlLYWWsWi4FZarEUQtFhsCBCiXCSIOYpQLkGUIAgpv/3HmsQQErKSrLWSwe/79cprZp41M89vVjK/POtZM8+YuyMiIuFTr7YDEBGR6lECFxEJKSVwEZGQUgIXEQkpJXARkZBqkMjKWrdu7SkpKYmsUkQk9NavX3/A3ZPLlic0gaekpJCdnZ3IKkVEQs/M/n955epCEREJKSVwEZGQiiqBm1mSmS02s21mttXM+ppZSzN73czygun58Q5WRES+Em0f+C+B19z9RjNrBDQFfgqsdPdZZjYDmAHcHac4RQQ4ceIEu3bt4tixY7UdisRBkyZN6NChAw0bNoxq/UoTuJk1BwYAYwHc/Thw3MxGAgOD1RYAq1ECF4mrXbt20axZM1JSUjCz2g5HYsjdKSgoYNeuXXTq1CmqbaLpQukM7Ad+Z2Ybzew3ZnYu0Nbd9wQV7wHalLexmU0ws2wzy96/f390RyIi5Tp27BitWrVS8j4LmRmtWrWq0qeraBJ4A6AX8LS7pwNHiHSXRMXd57l7hrtnJCefdhmjiFSRkvfZq6q/22gS+C5gl7v/I1heTCSh7zWzdkGl7YB9VapZRERqpNI+cHf/p5l9bGYXu/v7wBBgS/AzBpgVTF+Ja6QicprHX98e0/1NHXpRTPcXrfz8fP7+979zyy23AJCdnc3vf/97Zs+enbAYhg8fzsKFC0lKSqrytjk5OezevZvhw4fHPrAziPYqlJ8AzwVXoOwEfkSk9f6Cmd0KfATcFJ8QRUJi1b/Fv46kQfD5HmjeLv51JVB+fj4LFy4sSeAZGRlkZGQkNIZXX3212tvm5OSQnZ2d8AQe1XXg7p4T9GOnuvv17v6puxe4+xB37xJMD8Y7WBGpXfn5+VxyySWMHz+e7t27M3r0aFasWEG/fv3o0qULWVlZABw5coRx48bRu3dv0tPTeeWVyAf0f/3rX0yfPp3evXuTmprKr371KwBmzJjB3/72N9LS0nj88cdZvXo11157LQD3338/48aNY+DAgXTu3PmUVvmDDz7IJZdcwtChQxk1ahSPPvroaTGPHTuWiRMnMmjQIDp37sybb77JuHHj6Nq1K2PHji1ZLyUlhQMHDpCfn0/Xrl257bbb6NatG5mZmRw9ehSAgQMHlgwHcuDAAVJSUjh+/Dj33nsvixYtIi0tjUWLFlV4/Js3b6ZPnz6kpaWRmppKXl5ejX4fuhNTRKpkx44dTJ48mdzcXLZt28bChQtZs2YNjz76KI888ggADz/8MIMHD2bdunWsWrWK6dOnc+TIEX7729/SokUL1q1bx7p16/j1r3/Nhx9+yKxZs+jfvz85OTlMnTr1tDq3bdvGX//6V7KysnjggQc4ceIE2dnZLFmyhI0bN/LSSy+dcZylTz/9lDfeeIPHH3+c6667jqlTp7J582beffddcnJyTls/Ly+PSZMmsXnzZpKSkliyZEmF+27UqBE///nPufnmm8nJyeHmm2+u8Pjnzp3L5MmTS1rsHTp0qPovoJSEDmYlUisS0bXxNdKpUyd69OgBQLdu3RgyZAhmRo8ePcjPzwdg+fLlLF26tKRFfOzYMT766COWL19Obm4uixcvBuCzzz4jLy+PRo0anbHOa665hsaNG9O4cWPatGnD3r17WbNmDSNHjuScc84B4Lrrrqtw++uuu64kxrZt254Sf35+PmlpaacdY3HZZZddVnJc0aro+Pv27cvDDz/Mrl27uOGGG+jSpUuV9luWEriIVEnjxo1L5uvVq1eyXK9ePYqKioDITSlLlizh4osvPmVbd2fOnDkMGzbslPLVq1dHXWf9+vUpKiqiKg9kLx1j2fiLYz5TfcVdKA0aNODkyZMAZ7xeu6Lj79q1K5dffjnLli1j2LBh/OY3v2Hw4MFRH0dZ6kIRkZgbNmwYc+bMKUmyGzduLCl/+umnOXHiBADbt2/nyJEjNGvWjMOHD1epjiuvvJI//elPHDt2jMLCQpYtWxbbgyhHSkoK69evByj5FAGcFn9Fx79z5046d+7MXXfdxYgRI8jNza1RPGqBi4RYbV32V5l77rmHKVOmkJqairuTkpLCn//8Z8aPH09+fj69evXC3UlOTubll18mNTWVBg0a0LNnT8aOHUt6enqldfTu3ZsRI0bQs2dPLrzwQjIyMmjRokVcj2vatGl873vf4w9/+MMpLedBgwYxa9Ys0tLSmDlzZoXHv2jRIp599lkaNmzIN77xDe69994axWNV+RhSUxkZGa4HOgigfulq2po0iK7fuvCsu4ywugoLCznvvPP44osvGDBgAPPmzaNXr161HVaNbN26la5du55SZmbr3f206yrVAheR0JowYQJbtmzh2LFjjBkzJvTJu6qUwEUktBYuXFjbIdQqJfC6LpFdDYNmJq4uEakxXYUiIhJSSuAiIiGlBC4iElLqAxcJs1h/R1IHvwdJSUkhOzub1q1b13YodY5a4CIiIaUELiJRO3LkCNdccw09e/ake/fuLFq0iJSUFO6++2769OlDnz592LFjBwD79+/nu9/9Lr1796Z37968/fbbJfuoaKjZadOm0aNHD1JTU5kzZ05JvXPmzKFXr1706NGDbdu2Jf7A6yglcBGJ2muvvUb79u3ZtGkT7733HldddRUAzZs3JysrizvvvJMpU6YAMHnyZKZOncq6detYsmQJ48ePByoeanbevHl8+OGHbNy4kdzcXEaPHl1Sb+vWrdmwYQMTJ04sd8zvryv1gYtI1Hr06MG0adO4++67ufbaa+nfvz8Ao0aNKpkWj+e9YsUKtmzZUrLt559/zuHDhyscanXFihXcfvvtNGgQSUstW7Ys2faGG24AIkO7vvTSS/E/0JBQAheRqF100UWsX7+eV199lZkzZ5KZmQmc+jT14vmTJ0+ydu3akvG6i51pqNmKnspePLxr8VCyEqEuFBGJ2u7du2natCk/+MEPmDZtGhs2bABg0aJFJdO+ffsCkJmZyZNPPlmybfGTbyoaajUzM5O5c+eWJOiDB/WUxsqoBS4SZgm+7O/dd99l+vTp1KtXj4YNG/L0009z44038uWXX3L55Zdz8uRJnn/+eQBmz57NpEmTSE1NpaioiAEDBjB37twzDjW7fft2UlNTadiwIbfddht33nlnQo8vbDScbF13to6FouFkq6UuDier67RjqyrDyaoLRUQkpNSFIiI1UtUH/krsqAUuIhJSSuAiIiGlBC4iElJR9YGbWT5wGPgXUOTuGWbWElgEpAD5wPfc/dP4hCkiImVV5UvMQe5+oNTyDGClu88ysxnB8t0xjU5EzuipnKdiur870u6o8jZPPPEEEyZMoGnTpjFZL17mz59PZmYm7du3r5X646EmXSgjgQXB/ALg+hpHIyKh88QTT/DFF1/EbL14mT9/Prt37661+uMh2gTuwHIzW29mE4Kytu6+ByCYtilvQzObYGbZZpa9f//+mkcsIrWm7HCyDzzwALt372bQoEEMGjQIgIkTJ5KRkUG3bt247777gMhdmWXXW758OX379qVXr17cdNNNFBYWApEbg37605/St29fMjIy2LBhA8OGDeNb3/oWc+fOLYnlF7/4Bb179yY1NbWknvz8fLp27cptt91Gt27dyMzM5OjRoyxevJjs7GxGjx5NWloaR48eTeTbFjfRJvB+7t4LuBqYZGYDoq3A3ee5e4a7ZyQnJ1crSBGpG8oOJztlyhTat2/PqlWrWLVqFRAZLjY7O5vc3FzefPNNcnNzueuuu05Z78CBAzz00EOsWLGCDRs2kJGRwWOPPVZSzwUXXMDatWvp378/Y8eOZfHixbzzzjvce++9QCT55+XlkZWVRU5ODuvXr+ett94CIC8vj0mTJrF582aSkpJYsmQJN954IxkZGTz33HPk5OScNsBWWEXVB+7uu4PpPjP7T6APsNfM2rn7HjNrB+yLY5wiUgdUNJxsaS+88ALz5s2jqKiIPXv2sGXLFlJTU09Z55133mHLli3069cPgOPHj5cMggUwYsSIkvoKCwtp1qwZzZo1o0mTJhw6dIjly5ezfPly0tPTASgsLCQvL4+OHTvSqVMn0tLSgMjws2fzjUaVJnAzOxeo5+6Hg/lM4OfAUmAMMCuYvhLPQEWk9lU0nGyxDz/8kEcffZR169Zx/vnnM3bsWI4dO3baftydoUOHlgx8VVbx8LH16tUrmS9eLioqwt2ZOXMmP/7xj0/ZLj8//5T169evf9Z0l5Qnmi6UtsAaM9sEZAHL3P01Iol7qJnlAUODZRE5i5U3nGyzZs04fPgwEHlow7nnnkuLFi3Yu3cvf/nLX0q2Lb3eFVdcwdtvv13y+LUvvviC7du3Rx3HsGHDeOaZZ0r6zT/55BP27TtzJ0Dp+s8WlbbA3X0n0LOc8gJgSDyCEpHoVOeyv5oobzjZtWvXcvXVV9OuXTtWrVpFeno63bp1o3PnziVdJAATJkw4Zb358+czatQovvzySwAeeughLrrooqjiyMzMZOvWrSXdLueddx7PPvss9evXr3CbsWPHcvvtt3POOeeU+6CJMNJwsnWdhpOVUuricLISWxpOVkTka0AJXEQkpJTARURCSglcRCSklMBFREJKCVxEJKT0TEyRENs/58mY7i/5J3ee8fVDhw6xcOFC7rij5sPODh8+nIULF5KUlFSdUAW1wEWkCg4dOsRTT1VvDPKyw8m++uqrSt41pAQuIlGbMWMGH3zwAWlpaUyfPr3cIV3LDjm7aNGicoeTTUlJ4cCBA+Tn53PJJZcwfvx4unfvzujRo1mxYgX9+vWjS5cuZGVlAXDw4EGuv/56UlNTueKKK8jNzQXg/vvvZ9y4cQwcOJDOnTsze/ZsAO655x5++ctflsT+s5/9rOS1s4W6UEQkarNmzeK9994jJyeH5cuXs3jxYrKysnB3RowYwVtvvcX+/ftp3749y5YtA+Czzz6jRYsWPPbYY6xatYrWrVuftt8dO3bw4osvMm/ePHr37s3ChQtZs2YNS5cu5ZFHHuHll1/mvvvuIz09nZdffpk33niDH/7wh+Tk5ACwbds2Vq1axeHDh7n44ouZOHEit956KzfccAOTJ0/m5MmT/PGPfyz5Z3C2UAIXkWqpaEjX/v37VzrkbFmdOnWiR48eAHTr1o0hQ4ZgZvTo0aNkONg1a9awZMkSAAYPHkxBQQGfffYZANdccw2NGzemcePGtGnThr1795KSkkKrVq3YuHEje/fuJT09nVatWsXhnag9SuAiUi0VDekKnDbkbPGDGCpSdsjY0sPJFhUVldRXlpmdtn39+vVLthk/fjzz58/nn//8J+PGjaviEdZ96gMXkaiVHpK1oiFdyxtytuy21TFgwACee+45AFavXk3r1q1p3rz5Gbf5zne+w2uvvca6desYNmxYteuuq9QCFwmxyi77i7VWrVrRr18/unfvztVXX80tt9xy2pCuO3bsOG3IWTh9ONmquv/++/nRj35EamoqTZs2ZcGCBZVu06hRIwYNGkRSUtIZh5oNKw0nW9dpOFkpRcPJVs3Jkyfp1asXL774Il26dKntcKKi4WRF5Gtvy5YtfPvb32bIkCGhSd5VpS4UETkrXXrppezcubO2w4grtcBFQsXLvRpDzg5V/d0qgYuESJOiQgo+P6IkfhZydwoKCmjSpEnU26gLRSREOhzJZdc/YX9hUW2HInHQpEkTOnToEPX6SuAiIdLQj9OpMBt6D63tUKQOUBeKiEhIKYGLiISUEriISEgpgYuIhFTUX2KaWX0gG/jE3a81s5bAIiAFyAe+5+6fxiNISRDd3i4SKlVpgU8GtpZangGsdPcuwMpgWUREEiSqBG5mHYBrgN+UKh4JFA8HtgC4PqaRiYjIGUXbhfIE8L+BZqXK2rr7HgB332Nmbcrb0MwmABMAOnbsWP1I6xJ1NUhtO1tHqZQqqbQFbmbXAvvcfX11KnD3ee6e4e4ZycnJ1dmFiIiUI5oWeD9ghJkNB5oAzc3sWWCvmbULWt/tgH3xDFRERE5VaQvc3We6ewd3TwG+D7zh7j8AlgJjgtXGAK/ELUoRETlNTa4DnwUMNbM8YGiwLCIiCVKlwazcfTWwOpgvAIbEPiQREYmG7sQUEQkpJXARkZBSAhcRCSklcBGRkFICFxEJKSVwEZGQUgIXEQkpJXARkZBSAhcRCSklcBGRkFICFxEJKSVwEZGQqtJgVvKV/X/OSVhdydemJawuEQkPtcBFREJKCVxEJKTUhSIidYce1lwlaoGLiISUEriISEgpgYuIhJQSuIhISCmBi4iElBK4iEhIKYGLiISUEriISEgpgYuIhJQSuIhISFWawM2siZllmdkmM9tsZg8E5S3N7HUzywum58c/XBERKRZNC/xLYLC79wTSgKvM7ApgBrDS3bsAK4NlERFJkEoTuEcUBosNgx8HRgILgvIFwPXxCFBERMoXVR+4mdU3sxxgH/C6u/8DaOvuewCCaZu4RSkiIqeJKoG7+7/cPQ3oAPQxs+7RVmBmE8ws28yy9+/fX80wRUSkrCpdheLuh4DVwFXAXjNrBxBM91WwzTx3z3D3jOTk5JpFKyIiJaK5CiXZzJKC+XOA/wFsA5YCY4LVxgCvxClGEREpRzRP5GkHLDCz+kQS/gvu/mczWwu8YGa3Ah8BN8UxThERKaPSBO7uuUB6OeUFwJB4BCUiIpXTnZgiIiGlBC4iElJ6Kn0dtuvTowDs2FmQ0Hr7dm6V0PpEEuWpQ7lfLeQ8lbB670i7Iy77VQtcRCSklMBFREJKXSgiFTjl43YC3ZGUWiv1VmjVv9V2BPHx4d8SU0+n/nHbtVrgIiIhpQQuIhJSSuAiIiGlPnCRGEtZsadG2+9vcjKq9ZKvTatRPRJ+aoGLiISUEriISEiFpgvl8de311rdU4deVGt1n8nSejvist+Nhz454+t17jI3ka8ptcBFREJKCVxEJKSUwEVEQio0feCSOB8fOnrG19cejN/oiBWNhBjv29orO+aqaHmsKOp1mzfRKZgoa3cW8HG92P2eo7XrgwLuSIvPvtUCFxEJKSVwEZGQ0ue3EGjydvmXUPa0gzGva9OVrStdJ16XLwIszY/fvuuiz8vpbll57MyXcRbblP9ljeq+IOmcknldGhpOaoGLiISUEriISEgpgYuIhJT6wKNQ9jb+Kz4qoMmnib8cSaqv55oDtR2CSMypBS4iElJK4CIiIVVpAjezC8xslZltNbPNZjY5KG9pZq+bWV4wPT/+4YqISLFoWuBFwP9y967AFcAkM7sUmAGsdPcuwMpgWUREEqTSBO7ue9x9QzB/GNgKfBMYCSwIVlsAXB+nGEVEpBxV6gM3sxQgHfgH0Nbd90AkyQNtYh6diIhUKOrLCM3sPGAJMMXdPzezaLebAEwA6NixY3VilARK5OV20dy2LyIVi6oFbmYNiSTv59z9paB4r5m1C15vB+wrb1t3n+fuGe6ekZycHIuYRUSE6K5CMeC3wFZ3f6zUS0uBMcH8GOCV2IcnIiIViaYLpR/wP4F3zSwnKPspMAt4wcxuBT4CbopLhHLW0t2RIjVTaQJ39zVARR3eQ2IbjoiIREt3YoqIhJQSuIhISCmBi4iElBK4iEhIKYGLiISUHuggIrXiqUO5Ca3v43pn30NY1AIXEQkpJXARkZBSAhcRCSn1gYtIQq3dWQCcnX3SiaYWuIhISCmBi4iElLpQRORrKxEjYnZqnMv+3U+S/JM7Y75vtcBFREJKCVxEJKSUwEVEQkp94CJfUx8f+uoyvrUHC2oxEqkutcBFREJKCVxEJKSUwEVEQkoJXEQkpJTARURCSglcRCSklMBFREJKCVxEJKSUwEVEQkp3YooIS+vtqO0QpBoqbYGb2TNmts/M3itV1tLMXjezvGB6fnzDFBGRsqLpQpkPXFWmbAaw0t27ACuDZRERSaBKE7i7vwUcLFM8ElgQzC8Aro9tWCIiUpnq9oG3dfc9AO6+x8zaVLSimU0AJgB07NixmtVVzYbPF8V1//vq7aCnnfo/7WJvGdc6RcpKxNNkim26snXC6pLoxf0qFHef5+4Z7p6RnJwc7+pERL42qpvA95pZO4Bgui92IYmISDSq24WyFBgDzAqmr8QsIhGpc9RdUzdFcxnh88Ba4GIz22VmtxJJ3EPNLA8YGiyLiEgCVdoCd/dRFbw0JMaxiIhIFehWehGRkFICFxEJKSVwEZGQUgIXEQkpjUYYI+9b2dEGRETiSy1wEZGQUgIXEQkpJXARkZBSH7iI1CmJvG0/7NQCFxEJKSVwEZGQUgIXEQkpJXARkZBSAhcRCSklcBGRkFICFxEJKSVwEZGQUgIXEQkpJXARkZBSAhcRCSklcBGRkFICFxEJKSVwEZGQUgIXEQkpJXARkZBSAhcRCakaJXAzu8rM3jezHWY2I1ZBiYhI5aqdwM2sPvAfwNXApcAoM7s0VoGJiMiZ1aQF3gfY4e473f048EdgZGzCEhGRytTkocbfBD4utbwLuLzsSmY2AZgQLBaa2fulXm4NhOEJpoozthRnbCnO2Ip5nD9mMdz1k5rs4sLyCmuSwK2cMj+twH0eMK/cHZhlu3tGDWJICMUZW4ozthRnbIUlTqhZF8ou4IJSyx2A3TULR0REolWTBL4O6GJmncysEfB9YGlswhIRkcpUuwvF3YvM7E7gr0B94Bl331zF3ZTbtVIHKc7YUpyxpThjKyxxYu6ndVuLiEgI6E5MEZGQUgIXEQmpmCZwM3vGzPaZ2Xulynqa2Voze9fM/mRmzYPyRmb2u6B8k5kNDMqbmtkyM9tmZpvNbFYsY4xVnKVem2dm24N4vxvjOC8ws1VmtjV4LyYH5S3N7HUzywum55faZmYwtMH7ZjasVPllwTHsMLPZZlbeZaC1Hmep15eW/v3UtTjNbFTwfuaa2Wtm1rq24jSzVsH6hWb2ZKn9xPVcilWcwWtxO5eqEedQM1sf/H7Xm9ngUvuK23lULe4esx9gANALeK9U2Trgvwfz44AHg/lJwO+C+TbAeiL/UJoCg4LyRsDfgKvrWpzB8gPAQ8F8PaB1jONsB/QK5psB24kMW/B/gRlB+Qzg34P5S4FNQGOgE/ABUD94LQvoS+T6/b/E8j2NZZzB6zcAC0v/fupSnES+/N9X/PsOtr+/FuM8F7gSuB14stR+4nouxSrOeJ9L1YgzHWgfzHcHPim1r7idR9U6tpjvEFI4NTF+zldfll4AbAnm/wP4Qan1VgJ9ytnfL4Hb6mKcRO5EPTdhvyx4BRgKvA+0K/XH+X4wPxOYWWr9vwZ/bO2AbaXKRwG/qmtxBvPnAWuCEyymCTyG72dDYD+Ru+MMmAtMqK04S603ljKJsczrcTmXYhFnIs+laOMMyg0oIPJPPKHnUTQ/iegDfw8YEczfxFc3/2wCRppZAzPrBFzGqTcGYWZJwHVEkmadijOIDeBBM9tgZi+aWdt4BWdmKURaBv8A2rr7HoBg2iZYrbzhDb4Z/Owqp7yuxQnwIPD/gC/iEV8s4nT3E8BE4F0iN69dCvy2FuOMZj9JxPFcqkmciTyXqhHnd4GN7v4lCTyPopWIBD4OmGRm64l8fDkelD9D5A3IBp4A/g4UFW9kZg2A54HZ7r6zDsbZgMjdp2+7ey9gLfBoPAIzs/OAJcAUd//8TKuWU+ZnKI+pmsZpZmnAt939P2Md2ymV1zzOhkQSeDrQHsgl0lqvrTgr209cz6UYxJmQc6mqcZpZN+DfgR8XF5WzWq1ehx33BO7u29w9090vI/JH9EFQXuTuU909zd1HAklAXqlN5wF57v5EvGOsZpwFRFqJxcnmRSL96jEVJIslwHPu/lJQvNfM2gWvtyPSHwsVD2+wK5gvW17X4uwLXGZm+US6US4ys9V1MM40AHf/wCOfpV8A/lstxlmZuJ1LMYoz7udSVeM0sw5BPD909w+C4rifR1UV9wRuZm2CaT3g/xDpLyz+hvzcYH4oUOTuW4Llh4AWwJR4x1fdOIMT90/AwGAXQ4AtMY7JiHw03+ruj5V6aSkwJpgfQ6RPr7j8+2bWOOju6QJkBR8PD5vZFcE+f1hqm7oU59Pu3t7dU4h82bXd3QfWtTiBT4BLzSw5WG8osLUW4zzTvuJ2LsUqznifS1WNM+jSWUbk+4+3S8UZ1/OoWmL85cDzwB7gBJH/VrcCk4l867sdmMVXXxSmEPkSYSuwArgwKO9A5GPJViAn+Blf1+IMXrsQeIvIR+iVQMcYx3ll8F7klnovhgOtgvrygmnLUtv8jMinh/cp9Q05kEGkn/8D4Mni46trcZZ6PYXYX4USy/fz9uBvIpdI8mlVy3HmAweBwuBv+tJ4n0uxijPe51JV4yTSgDtSat0coE28z6Pq/OhWehGRkNKdmCIiIaUELiISUkrgIiIhpQQuIhJSSuAiIiGlBC4iElJK4CIiIfVfpxKtYhzSZgYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show timing of types -- drop meeting transcripts due to delay in release, press conference transcripts as only began in 2011\n",
    "plt.hist(text_data[text_data.type == 'minutes'].date, alpha = 0.5, label = 'meeting minutes')\n",
    "plt.hist(text_data[text_data.type == 'speech'].date, alpha = 0.5, label = 'speech')\n",
    "plt.hist(text_data[text_data.type == 'statement'].date, alpha = 0.5, label = 'statement')\n",
    "plt.hist(text_data[text_data.type == 'testimony'].date, alpha = 0.5, label = 'testimony')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep only Meeting Minutes, Speech, Statement, Testimony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1042, 11)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_filtered = text_data[text_data.type.isin(['minutes', 'speech', 'statement', 'testimony'])]\n",
    "text_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/akalodzitsa/nltk_data'\n    - '/Users/akalodzitsa/opt/anaconda3/nltk_data'\n    - '/Users/akalodzitsa/opt/anaconda3/share/nltk_data'\n    - '/Users/akalodzitsa/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/corpus/util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/Users/akalodzitsa/nltk_data'\n    - '/Users/akalodzitsa/opt/anaconda3/nltk_data'\n    - '/Users/akalodzitsa/opt/anaconda3/share/nltk_data'\n    - '/Users/akalodzitsa/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m text_filtered[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_clean\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtext_filtered\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m text_filtered\u001b[38;5;241m.\u001b[39mtext_clean[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/series.py:4357\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4248\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4249\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4252\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4253\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FrameOrSeriesUnion:\n\u001b[1;32m   4254\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4255\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4256\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4355\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4356\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/apply.py:1043\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1040\u001b[0m     \u001b[38;5;66;03m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m-> 1043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/apply.py:1098\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1092\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m   1093\u001b[0m         \u001b[38;5;66;03m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m         \u001b[38;5;66;03m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[1;32m   1095\u001b[0m         \u001b[38;5;66;03m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m         \u001b[38;5;66;03m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m         \u001b[38;5;66;03m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[0;32m-> 1098\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1101\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1105\u001b[0m     \u001b[38;5;66;03m# GH 25959 use pd.array instead of tolist\u001b[39;00m\n\u001b[1;32m   1106\u001b[0m     \u001b[38;5;66;03m# so extension arrays can be used\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(pd_array(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/lib.pyx:2859\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36mclean\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordNetLemmatizer\n\u001b[1;32m      5\u001b[0m exclude \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(string\u001b[38;5;241m.\u001b[39mpunctuation)\n\u001b[0;32m----> 6\u001b[0m stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[43mstopwords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      7\u001b[0m lemma \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[1;32m      9\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^A-z,.-]+\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/corpus/util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/corpus/util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/akalodzitsa/nltk_data'\n    - '/Users/akalodzitsa/opt/anaconda3/nltk_data'\n    - '/Users/akalodzitsa/opt/anaconda3/share/nltk_data'\n    - '/Users/akalodzitsa/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "text_filtered['text_clean'] = text_filtered['text'].apply(clean)\n",
    "\n",
    "text_filtered.text_clean[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'text_clean'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m corpus, id2word, bigram_text \u001b[38;5;241m=\u001b[39m get_corpus(\u001b[43mtext_filtered\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_clean\u001b[49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py:5487\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   5481\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[1;32m   5482\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[1;32m   5483\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[1;32m   5484\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   5485\u001b[0m ):\n\u001b[1;32m   5486\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[0;32m-> 5487\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'text_clean'"
     ]
    }
   ],
   "source": [
    "corpus, id2word, bigram_text = get_corpus(text_filtered.text_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'id2word' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_list, coherence_values \u001b[38;5;241m=\u001b[39m compute_coherence_values(dictionary\u001b[38;5;241m=\u001b[39m\u001b[43mid2word\u001b[49m, corpus\u001b[38;5;241m=\u001b[39mcorpus, texts\u001b[38;5;241m=\u001b[39mbigram_text, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'id2word' is not defined"
     ]
    }
   ],
   "source": [
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=bigram_text, start=5, limit=30, step=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'coherence_values' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m; start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m; step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m;\n\u001b[1;32m      3\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(start, limit, step)\n\u001b[0;32m----> 4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(x, \u001b[43mcoherence_values\u001b[49m)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNum Topics\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCoherence score\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'coherence_values' is not defined"
     ]
    }
   ],
   "source": [
    "# Coherence score plot\n",
    "limit=30; start=5; step=2;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose 8 topics -- highest coherence score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Topics\n",
    "num_topics = 8\n",
    "# Build LDA model\n",
    "lda_model_8 = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                        id2word=id2word,\n",
    "                                        num_topics=num_topics, \n",
    "                                        passes=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words in Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "cloud = WordCloud(stopwords=stop_words,\n",
    "                  background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=10,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10,10), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Keyword in the topics\n",
    "pprint(lda_model_8.print_topics())\n",
    "doc_lda = lda_model_8[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess topic coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model_8.log_perplexity(corpus))  # Measure of how good the model is. Lower is better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "from gensim.models import CoherenceModel\n",
    "coherence_model_lda = CoherenceModel(model=lda_model_8, texts=bigram_text, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda) # Want above .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZE LDA OUTPUT AS HTML-PAGE\n",
    "import pyLDAvis.gensim_models\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model_8, corpus, dictionary=lda_model_8.id2word)\n",
    "vis\n",
    "\n",
    "# output results\n",
    "pyLDAvis.save_html(vis, '../img/bigram.lda8-50.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add LDA Topics to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD LDA TOPICS TO DATAFRAME\n",
    "# REFERENCE CODE: https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0\n",
    "\n",
    "data = text_filtered['text_clean']\n",
    "def format_topics_sentences(ldamodel=lda_model_8, corpus=corpus, texts=data):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "# Run function to create new df\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model_8, corpus=corpus, texts=bigram_text)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "df_dominant_topic.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This command prints out the topics in a text and the percent of the text from each topic\n",
    "\n",
    "# if you want to make a list for each doc-topic pair\n",
    "l = [lda_model_8.get_document_topics(item) for item in corpus]\n",
    "\n",
    "# Create a list of lists (list of topics inside each doc)\n",
    "topic_distribution = list()\n",
    "for num in range(len(text_filtered['text_clean'])):\n",
    "    props = [0] * 18  # create empty list\n",
    "    doc = num\n",
    "    for item in lda_model_8[corpus[num]]:\n",
    "        topic = item[0]\n",
    "        props[topic] = item[1]\n",
    "    topic_distribution.append(props)\n",
    "    \n",
    "# Turn list of lists into dataframe\n",
    "topic_dist = pd.DataFrame(topic_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine topics across transcript, minutes, etc. for each meeting date\n",
    "final_text = text_filtered.join(topic_dist)\n",
    "final_text = final_text.groupby('next_meeting').mean()\n",
    "\n",
    "print(final_text.shape)\n",
    "final_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find na's\n",
    "na_rows = final_text[final_text.isna().any(axis=1)]\n",
    "\n",
    "print(na_rows.shape)\n",
    "display(na_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_text = final_text.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join LDA and nontext data\n",
    "joined_data = final_text.join(nontext_data)\n",
    "\n",
    "print(joined_data.shape)\n",
    "joined_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(joined_data.index, joined_data.next_decision, 'o', label = 'Next Decision')\n",
    "#plt.plot(joined_data.index, joined_data[0], 'o', label = 'Topic 0')\n",
    "#plt.plot(joined_data.index, joined_data[1], 'o', label = 'Topic 1')\n",
    "#plt.plot(joined_data.index, joined_data[2], 'o', label = 'Topic 2')\n",
    "#plt.plot(joined_data.index, joined_data[3], 'o', label = 'Topic 3')\n",
    "#plt.plot(joined_data.index, joined_data[4], 'o', label = 'Topic 4')\n",
    "plt.plot(joined_data.index, joined_data[5], 'o', label = 'Topic 5')\n",
    "#plt.plot(joined_data.index, joined_data[6], 'o', label = 'Topic 6')\n",
    "#plt.plot(joined_data.index, joined_data[7], 'o', label = 'Topic 7')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify rate decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change target variable from -1 to 1 to 1 to 3\n",
    "def convert_class(x):\n",
    "    if x == 1:\n",
    "        return 3\n",
    "    elif x == 0:\n",
    "        return 2\n",
    "    elif x == -1:\n",
    "        return 1\n",
    "\n",
    "joined_data['RateDecision'] = joined_data.RateDecision.apply(convert_class)\n",
    "joined_data['next_decision'] = joined_data['RateDecision'].shift(1)\n",
    "\n",
    "# drop NA caused by shifting\n",
    "joined_data = joined_data.dropna(subset = ['next_decision'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(df, file_name, dir_name='../data/'):\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.mkdir(dir_name)\n",
    "        \n",
    "    # Save results to a picke file\n",
    "    file = open(dir_name + file_name + '.pickle', 'wb')\n",
    "    pickle.dump(df, file)\n",
    "    file.close()\n",
    "\n",
    "    # Save results to a csv file\n",
    "    df.to_csv(dir_name + file_name + '.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Text Data\n",
    "save_data(final_text, 'text_w_lda')\n",
    "\n",
    "# Final joined data\n",
    "save_data(joined_data, 'final_fed_data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
